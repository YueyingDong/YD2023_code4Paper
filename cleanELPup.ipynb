{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# purpose: clean and parse the pupil data\n",
    "previous: readEL.ipynb\n",
    "\n",
    "clean:\n",
    "- readin the pupil files, reject trials where no response is made\n",
    "- clean it using speed deviation (see deBlink function), interpolate using cubic spline or linear methods; smooth the resulting array; Check for outliers and reject them\n",
    "- reject any trial/subject if too many data is missing\n",
    "\n",
    "parse:\n",
    "- readin the event dataframe\n",
    "- find the time onset of retrocue, for each trial. Use that to identify the baseline and epoch of interest (baseline is 100ms; epoch of interest is 3000ms after cue onset)\n",
    "- if any nan occur during baseline, reject trial\n",
    "- normalize the epoch of interest data using baseline mean for every trial\n",
    "- add trial tags (the block reliability, cued item condition, and subject id to the data)\n",
    "\n",
    "save: \n",
    "- save the psychopy files after rejecting missing response trials\n",
    "- seperately save a blink mask\n",
    "- save the normalized retrocue period and the baseline perid data;\n",
    "\n",
    "\n",
    "upnext: cleanELSacc.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#for handling RAM\n",
    "import gc \n",
    "\n",
    "#stats\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.formula.api import ols\n",
    "import pingouin as pg\n",
    "\n",
    "\n",
    "#my to go packages\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib import cm\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "\n",
    "#for readin\n",
    "import glob # Package for Unix-style pathname pattern expansion\n",
    "import os   # Python operating system interface\n",
    "\n",
    "#for signal processing\n",
    "from scipy.interpolate import CubicSpline,interp1d\n",
    "import scipy.signal as signal\n",
    "import scipy.stats as stats\n",
    "from scipy.signal import filtfilt, butter\n",
    "from scipy.fft import rfft, rfftfreq\n",
    "from scipy.stats import sem \n",
    "from scipy.stats import norm\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "#other packages\n",
    "import ast\n",
    "import itertools\n",
    "from itertools import groupby\n",
    "from more_itertools import consecutive_groups\n",
    "import more_itertools as mit\n",
    "from operator import itemgetter\n",
    "import statistics\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions for speed array\n",
    "def find_abs_speed_noSmooth(timepoint,ps): #find the raw speed array without applying any smoothing\n",
    "\n",
    "    ds = np.zeros(len(ps))\n",
    "\n",
    "    try:\n",
    "        ps = pd.to_numeric(ps, errors='coerce').fillna(0.0) #repleace non numeric values with 0\n",
    "    except AttributeError:\n",
    "        ps = np.where(np.isnan(ps),0,ps)\n",
    "    \n",
    "    try:\n",
    "        ps = ps.values\n",
    "        timepoint = timepoint.values\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    prev = abs((ps[1:-1] - ps[0:-2])/(timepoint[1:-1] -timepoint[0:-2]))\n",
    "    post = abs((ps[1:-1] -ps[2:])/ (timepoint[1:-1] -timepoint[2:]))\n",
    "               \n",
    "    ds[1:-1] = np.where(prev >= post,prev,post)\n",
    "    ds[0] = abs((ps[1] - ps[0])/ (timepoint[1] - timepoint[0]))\n",
    "    ds[-1] = abs((ps[-1] - ps[-2])/ (timepoint[-1] - timepoint[-2]))\n",
    "    \n",
    "    return ds\n",
    "def find_abs_speed_pup(ps): #this is same as find_abs_speed, but now using a smoothed speed array for pupil\n",
    "    #by using 5 points to calculate speed instead of 2\n",
    "    ds = np.zeros(len(ps))\n",
    "\n",
    "    try:\n",
    "        ps = pd.to_numeric(ps, errors='coerce').fillna(0.0) #repleace non numeric values with 0\n",
    "    except AttributeError:\n",
    "        ps = np.where(np.isnan(ps),0,ps)\n",
    "    \n",
    "    try:\n",
    "        nminus1 = ps.iloc[1:-3].values\n",
    "        nplus1 = ps.iloc[3:-1].values\n",
    "        nminus2 = ps.iloc[0:-4].values\n",
    "        nplus2 = ps.iloc[4:].values\n",
    "\n",
    "        ds[0] = abs(ps.iloc[1] - ps.iloc[0])\n",
    "        ds[1] = abs(ps.iloc[2] - ps.iloc[1])\n",
    "        ds[-1] = abs(ps.iloc[-1] - ps.iloc[-2])\n",
    "        ds[-2] = abs(ps.iloc[-2] - ps.iloc[-3])\n",
    "    except AttributeError:\n",
    "        nminus1 = ps[1:-3]\n",
    "        nplus1 = ps[3:-1]\n",
    "        nminus2 = ps[0:-4]\n",
    "        nplus2 = ps[4:]\n",
    "\n",
    "        ds[0] = abs(ps[1] - ps[0])\n",
    "        ds[1] = abs(ps[2] - ps[1])\n",
    "        ds[-1] = abs(ps[-1] - ps[-2])\n",
    "        ds[-2] = abs(ps[-2] - ps[-3])\n",
    "               \n",
    "    ds[2:-2] = abs((nplus1 + nplus2- nminus1 - nminus2  )/6)\n",
    "    \n",
    "    \n",
    "    return ds\n",
    "def find_abs_speed_sacc(vec):#this is same as find_abs_speed, but now using a smoothed speed array,for saccade \n",
    "    # because we're using.iloc and .values, need to make sure the \n",
    "    # vec input is from dataframe\n",
    "    try:\n",
    "        nminus1 = vec.iloc[1:-3].values\n",
    "        nplus1 = vec.iloc[3:-1].values\n",
    "        nminus2 = vec.iloc[0:-4].values\n",
    "        nplus2 = vec.iloc[4:].values\n",
    "    except AttributeError:\n",
    "        nminus1 = vec[1:-3]\n",
    "        nplus1 = vec[3:-1]\n",
    "        nminus2 = vec[0:-4]\n",
    "        nplus2 = vec[4:]\n",
    "    #calculate derivative\n",
    "    d = (nplus1 + nplus2- nminus1 - nminus2  )/6\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#other helper functions\n",
    "def consecutive(data, stepsize=10,\n",
    "                findSame = False): # helper function for findGazeShift, deBlink, and more\n",
    "    \"\"\"\n",
    "    @ data = row vector\n",
    "    @ stepsize =  the larger the step size, the more difference it allows for two groups\n",
    "                to be considered within the same cluster\n",
    "    @ findSame = if True, find concecutive data point of the same value, this is set true\n",
    "                 for finding blink counts using the blink mask (a df of 0 and 1 )\n",
    "    \"\"\"\n",
    "    # this will split an array of numbers into many subarrays of consecutive numbers\n",
    "    # the stepsize controls for how close of the two numbers for them to be considered\n",
    "    # in the same cluster; e.g. [1,2,3,4,5,10] would be clustered together if stepsize = 5\n",
    "\n",
    "    #data is the array of timepoint where the saccade velocity array crossed the threshold\n",
    "    if findSame:\n",
    "        stepsize = 0\n",
    "        return np.split(data, np.where(abs(np.diff(data)) > stepsize)[0]+1)\n",
    "    return np.split(data, np.where(np.diff(data) > stepsize)[0]+1)\n",
    "def smooth(x,window_len=11,window='hanning'): #smoothing function\n",
    "    try:\n",
    "        x = x.values\n",
    "    except:\n",
    "        pass\n",
    "    # about np windows:\n",
    "    # https://numpy.org/doc/stable/reference/routines.window.html\n",
    "    if x.ndim != 1:\n",
    "        raise ValueError(\"smooth only accepts 1 dimension arrays.\")\n",
    "\n",
    "    if x.size < window_len:\n",
    "        raise ValueError(\"Input vector needs to be bigger than window size.\")\n",
    "\n",
    "    if window_len<3:\n",
    "        return x\n",
    "\n",
    "    if not window in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:\n",
    "        raise ValueError(\"Window is on of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\")\n",
    "\n",
    "    #this is to prepare the input for convolve i.e. add 1/2 window length padding before and after\n",
    "    s=np.r_[x[window_len-1:0:-1],x,x[-2:-window_len-1:-1]] \n",
    "    \n",
    "    if window == 'flat': #moving average\n",
    "        w=np.ones(window_len,'d')\n",
    "    else:\n",
    "        w=eval('np.'+window+'(window_len)')\n",
    "\n",
    "    #sliding window\n",
    "    y=np.convolve(w/w.sum(),s,mode='valid')\n",
    "    \n",
    "    #get rid of the paddings\n",
    "    return y[int(window_len/2):int(-0.5*window_len)] #select the data points to get rid of the delay\n",
    "def strictly_increasing(L):#check if an array is uniformly increasing\n",
    "    return all(x<y for x, y in zip(L, L[1:]))\n",
    "def fs(width,height):#setting plot size\n",
    "    plt.rcParams['figure.figsize'] = (width,height)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function used for blink identification\n",
    "def mad_func(ds,mad_threshold): #calculate the median absolute diviation (MAD)\n",
    "    \n",
    "    \"\"\"\n",
    "    function that calculate the median absolute diviation (MAD)\n",
    "    ---------------------------------------------\n",
    "    input: \n",
    "    ds: median absolute dilation speed, which is calculated in dilation_speed_filter\n",
    "    mad_threshold: the threshold of median deviation (MAD); any data that is above median dilation speed + threshold*(MAD) will be rejected\n",
    "    ---------------------------------------------\n",
    "    output:\n",
    "    returns a float and a boolean, first is the threshold, second is the bool array\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ds = np.where(np.isnan(ds),0,ds)\n",
    "    except:\n",
    "        ds = np.where(ds.isnull(),0,ds)\n",
    "    mad = np.nanmedian(abs(ds - np.nanmedian(ds))) # median absolute deviation\n",
    "    rej_threshold = np.median(ds) + mad_threshold * mad # reject data above this threshold\n",
    "       \n",
    "    #returning an bool array, true = rejected, false = keep\n",
    "    return (abs(ds) >= rej_threshold)  , rej_threshold\n",
    "def reject_outlier(pt,rejarr, minGapLen = 0.03, borderlen = 0.05, borderlenBefore = 0,\n",
    "                   freq = 1000): # \n",
    "    \"\"\"\n",
    "  \n",
    "    ---------------------------------------------\n",
    "    input:\n",
    "    pt: pupil timestamps from pup_raw\n",
    "    rejarr: pupil diameter with nans, assigning nan to the pupil array using the output of dilation_speed_filter()\n",
    "    ---------------------------------------------\n",
    "    output: return a dictionary, each key is the starting time of the nan value, each value is the end time of nan\n",
    "    \n",
    "    \"\"\"\n",
    "    borderN = borderlen* freq #number of adjacent data points that needs to be rejected\n",
    "    borderNbefore = borderlenBefore * freq\n",
    "    gapMinN = minGapLen * freq #number of data points in the gap\n",
    "    \n",
    "    b = np.where(rejarr==True, np.nan,rejarr)\n",
    "    idx0 = np.flatnonzero(np.r_[True, np.diff(np.isnan(b))!=0,True])\n",
    "    count = np.diff(idx0)\n",
    "    idx = idx0[:-1]\n",
    "    valid_mask =  np.isnan(b[idx])\n",
    "    out_idx = idx[valid_mask]\n",
    "    out_count = count[valid_mask]\n",
    "    m = out_idx + out_count-1\n",
    "    out = list(zip(out_idx, m))\n",
    "\n",
    "    if len(out) == 0:\n",
    "        return out\n",
    "\n",
    "    #if the number of consecutively rejected data satisfies as a gap, record it\n",
    "    outbool = np.array([(out[i][-1] - out[i][0])> gapMinN for i in range(len(out))])\n",
    "    conRejarr = np.array(out)[outbool]\n",
    "    gapCorrarr = [[]]*len(conRejarr)\n",
    "    \n",
    "    for i in range(len(conRejarr)):\n",
    "        conRejarr[i][0] = max(0,conRejarr[i][0]-borderNbefore)\n",
    "        #print(conRejarr[i][-1],conRejarr[i][-1]+borderN)\n",
    "        conRejarr[i][-1] = min(len(pt)-1,conRejarr[i][-1]+borderN)\n",
    "        gapCorrarr[i] = np.arange(conRejarr[i][0],conRejarr[i][-1]+1)\n",
    "   \n",
    "    return np.array(gapCorrarr)\n",
    "def blkLen(rowInput,blkLen):#counts how many blink has occured using the mask df\n",
    "    #blink len default = how many consecutive points for one event to be registered as blink\n",
    "    #count this as 1 blink event\n",
    "    boolMask = [list(g)[0]>0 for k, g in groupby(rowInput) ]\n",
    "    repeatingVal = np.array([list(g) for k, g in groupby(rowInput)])\n",
    "    \n",
    "    blkArr = repeatingVal[boolMask]\n",
    "    blkCnt = sum([len(i) > blkLen for i in blkArr])\n",
    "    return blkCnt\n",
    "def medianVariance(vec):#vec is the result from find_abs_speed_pup/sacc\n",
    "    try:\n",
    "        val = np.nanmedian(abs(vec - np.nanmedian(vec)))\n",
    "    except:\n",
    "        print('F**k! medianVariance function encountered an error:')\n",
    "        print('dtype of vec:' + str(type(vec)))\n",
    "        print('np.nanmedian(vec) :' + str(np.nanmedian(vec)))\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#older functions\n",
    "def interpolate_blink(paddedRejIndex,paddedRejBool, timestamp,pupilsize,\n",
    "                      maxGapLen = 0.5,freq = 200):#paddedRejIndex = output from reject_outlier; ..Bool is the pseudodf['paddedDsRej']\n",
    "    # check for the length for each gap, if it's longer than the maximum length defined for a gap\n",
    "    # then don't interpolate, just leave it blank\n",
    "    interpolateIndex = paddedRejIndex[[len(i)<maxGapLen*freq for i in paddedRejIndex]]\n",
    "    \"\"\" i found that it works better to not smooth the pupil size\n",
    "     \"\"\"\n",
    "    pupilsize = np.array(smooth(pupilsize))\n",
    "   \n",
    "    #for each blink, select the onset/offset pupil size, as well as two equal distant time point,call it ref list\n",
    "    refList = np.array([np.array([max(0,i[0] - len(i)),i[0],i[-1],min(len(timestamp)-1,i[-1] + len(i))]) for i in interpolateIndex])\n",
    "    \n",
    "    #use the refList to retrieve the paddedRejIndex, if more than 2 values are True, i.e. the first and last reference points\n",
    "    #are also rejected data, then use the linear interpolation, else if the first and last ref points can be retrieved,\n",
    "    #use cubic spline interpolation\n",
    "    linearList = refList[np.sum(paddedRejBool.values[refList],axis = 1) > 2]\n",
    "    cubicList = refList[np.sum(paddedRejBool.values[refList],axis = 1)== 2]\n",
    "    \n",
    "    #use the linearlist or cubic list, find the timestamp or pupilsize values used for interpolation\n",
    "    cubictss = timestamp.values[cubicList]\n",
    "    cubicarr = pupilsize[cubicList]#pupilsize.values[cubicList]\n",
    "    \n",
    "    lineartss = timestamp.values[linearList]\n",
    "    lineararr = pupilsize[linearList]#pupilsize.values[linearList]\n",
    "    \n",
    "    #calculate the interpolated values; Index is the array of timestamp arrays, FuncList is the interpolated value\n",
    "    cubicIndex = [np.array(timestamp.iloc[cubicList[i][1]:cubicList[i][2]]) for i in range(len(cubicList))]\n",
    "    csFuncList = [CubicSpline(cubictss[i],cubicarr[i])(cubicIndex[i]) for i in range(len(cubicList))]\n",
    "\n",
    "    linearIndex = [np.array(timestamp.iloc[linearList[i][1]:linearList[i][2]]) for i in range(len(linearList))]\n",
    "    lFuncList = [interp1d(lineartss[i],lineararr[i])(linearIndex[i]) for i in range(len(linearList))]\n",
    "   \n",
    "    return cubicIndex,csFuncList,linearIndex,lFuncList\n",
    "def dsclean(thisdf,arr):\n",
    "    \"\"\"\n",
    "    this is the main function for reject, clean, and interpolate\n",
    "    thisdf is the df of the specific subject\n",
    "    #1st it, arr = 'diameter_3d', 2+ it, arr = 'diameter_3dNew'\n",
    "    \n",
    "    note: outside the function, these four variables needs to be specified, e.g.\n",
    "    maxGapLen = 1; minGapLen = 0.0 ; freq = 1000; gapMinN = minGapLen*freq; gapMaxN = maxGapLen*freq\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    #1st ite\n",
    "    arr = thisdf[arr] #1st it, arr = diameter_3d, 2+ it, arr = diameter_3dNew\n",
    "    rejarr,rejThreshold = mad_func(find_abs_speed(thisdf.pupil_timestamp,arr),3.5)    #need to get rid of zeros\n",
    "    thisdf['dsRej'] = (rejarr == True) | (arr.isnull()) | (arr == 0)\n",
    "\n",
    "    paddedDsRej = reject_outlier(thisdf.pupil_timestamp,thisdf.dsRej,minGapLen = minGapLen,borderlen=0.025,freq= 1000)\n",
    "    thisdf['paddedDsRej'] = np.where(thisdf.index.isin(np.concatenate(paddedDsRej)),True,False)\n",
    "\n",
    "    gapCorrarr = np.where(thisdf.paddedDsRej)[0]\n",
    "    #the sum of sublist in gapCorrarrparsed is the same length as the gapCorrarr\n",
    "    gapCorrarrParsed = [np.array(list(group)) for group in mit.consecutive_groups(gapCorrarr)] \n",
    "\n",
    "    #if the gap is shorter than minimum length, don't count it as a gap (but i've set min gap len to 0)\n",
    "    outbool = np.array([(gapCorrarrParsed[i][-1] - gapCorrarrParsed[i][0])> gapMinN for i in range(len(gapCorrarrParsed))])\n",
    "    gapCorrarrParsed = np.array(gapCorrarrParsed)[outbool]\n",
    "    #correct the paddedDsRej, excluding shorter-than-gap values, i.e. set those to false\n",
    "    thisdf['paddedDsRejNoGap'] = np.where(thisdf.index.isin(np.concatenate(gapCorrarrParsed)),True,False)\n",
    "\n",
    "    #also don't interpolate for gap length longer than maximum gap len\n",
    "    #first iteration use diameter 3d\n",
    "    csts,csi,lts,li = interpolate_blink(paddedRejIndex = gapCorrarrParsed,\n",
    "                                        paddedRejBool = thisdf['paddedDsRejNoGap'],\n",
    "                                        timestamp = thisdf.pupil_timestamp,\n",
    "                                        pupilsize = arr,\n",
    "                                        freq = 1000) #this pupilsize is different for iteretions\n",
    "\n",
    "    #for this 1st iteration, make a copy of diameter 3d to add the interpolated data\n",
    "    thisdf['diameterThisIter'] = arr #this pupilsize is different for iteretions\n",
    "    try:\n",
    "        thisdf.loc[np.where(thisdf['pupil_timestamp'].isin(np.concatenate(csts)))[0],'diameterThisIter'] = np.concatenate(csi)\n",
    "        thisdf.loc[np.where(thisdf['pupil_timestamp'].isin(np.concatenate(lts)))[0],'diameterThisIter'] = np.concatenate(li)\n",
    "    except ValueError: \n",
    "        print('no array to concat')\n",
    "    \"\"\"\n",
    "    minPup = np.mean(thisdf['diameterThisIter']) - 3*np.std(thisdf['diameterThisIter'])\n",
    "    maxPup = np.mean(thisdf['diameterThisIter']) + 3*np.std(thisdf['diameterThisIter'])\n",
    "\n",
    "    #also reject periods of pupil size that's larger the 3 sd away from the median, callit diameter_3dNew\n",
    "    thisdf['diameter_3dNew'] = np.where(((thisdf['diameterThisIter']<minPup)|(thisdf['diameterThisIter']>maxPup)),0, thisdf['diameterThisIter'] )\n",
    "    \"\"\"\n",
    "    return thisdf\n",
    "def tlclean(thisdf,windown = 501,mad = 10,\n",
    "            psCol = 'diameterThisIter',tsCol = 'pupil_timestamp',psColNew = 'diameter_3dNew'):\n",
    "    \n",
    "    #make trendline by interpolating and smoothing\n",
    "    cs = CubicSpline(thisdf[tsCol][~thisdf[psCol].isnull()].values,\n",
    "                   thisdf[psCol][~thisdf[psCol].isnull()].values)\n",
    "    interp = cs(thisdf[tsCol])\n",
    "    smoothed = smooth(interp,windown)\n",
    "    #tlmad = bool list\n",
    "    tlmad = mad_func(thisdf[psCol] -smoothed,mad)[0]\n",
    "    thisdf['dsRej'] = (tlmad == True) | (thisdf[psCol].isnull() == True) | (thisdf[psCol] == 0)\n",
    "\n",
    "    #don't pad the tl deviations\n",
    "    arr = thisdf[psCol]\n",
    "    gapCorrarr = (np.where(thisdf.dsRej)[0]) \n",
    "    #the sum of sublist in gapCorrarrparsed is the same length as the gapCorrarr\n",
    "    gapCorrarrParsed = np.array([np.array(list(group)) for group in mit.consecutive_groups(gapCorrarr)]) \n",
    "\n",
    "    #preparation for interpolation\n",
    "    interpolateIndex = gapCorrarrParsed[[len(i)<maxGapLen*freq for i in gapCorrarrParsed]]\n",
    "    refList = np.array([np.array([max(0,i[0] - len(i)),i[0],i[-1],min(len(thisdf[tsCol])-1,i[-1] + len(i))]) for i in interpolateIndex])\n",
    "    increasingList = np.array([strictly_increasing(i) for i in refList])\n",
    "\n",
    "    linearList = refList[(np.sum(thisdf['dsRej'].values[refList],axis = 1) > 2) | (increasingList== False) ]\n",
    "    cubicList = refList[(np.sum(thisdf['dsRej'].values[refList],axis = 1)== 2) & (increasingList)]\n",
    "\n",
    "    #use the linearlist or cubic list, find the thisdf[tsCol] or arr values used for interpolation\n",
    "    cubictss = thisdf[tsCol].values[cubicList]\n",
    "    cubicarr = arr.values[cubicList]\n",
    "    #correct for nan\n",
    "    cubicList = cubicList[np.isnan(cubicarr).sum(axis = 1) !=1]##\n",
    "    cubictss = cubictss[np.isnan(cubicarr).sum(axis = 1) !=1]##\n",
    "    cubicarr = cubicarr[np.isnan(cubicarr).sum(axis = 1) !=1] ##\n",
    "\n",
    "    #for linear\n",
    "    lineartss = thisdf[tsCol].values[linearList]\n",
    "    lineararr = arr.values[linearList]\n",
    "    #correct for nan\n",
    "    linearList = linearList[np.isnan(lineararr).sum(axis = 1) !=1]##\n",
    "    lineartss = lineartss[np.isnan(lineararr).sum(axis = 1) !=1]##\n",
    "    linearr = lineararr[np.isnan(lineararr).sum(axis = 1) !=1] ##\n",
    "\n",
    "\n",
    "    #calculate the interpolated values; Index is the array of thisdf[tsCol] arrays, FuncList is the interpolated value\n",
    "    cubicIndex = [np.array(thisdf[tsCol].iloc[cubicList[i][1]:cubicList[i][2]]) for i in range(len(cubicList))]\n",
    "    csFuncList = [CubicSpline(cubictss[i],cubicarr[i])(cubicIndex[i]) for i in range(len(cubicList))]\n",
    "\n",
    "    linearIndex = [np.array(thisdf[tsCol].iloc[linearList[i][1]:linearList[i][2]]) for i in range(len(linearList))]\n",
    "    lFuncList = [interp1d(lineartss[i],lineararr[i])(linearIndex[i]) for i in range(len(linearList))]\n",
    "\n",
    "    #for this 1st iteration, make a copy of diameter 3d to add the interpolated data\n",
    "    thisdf[psColNew] = arr #copy the old ps column, then modify it using the interpolated values\n",
    "    thisdf.loc[np.where(thisdf[tsCol].isin(np.concatenate(cubicIndex)))[0],psColNew] = np.concatenate(csFuncList)\n",
    "    thisdf.loc[np.where(thisdf[tsCol].isin(np.concatenate(linearIndex)))[0],psColNew] = np.concatenate(lFuncList)\n",
    "    \n",
    "    return thisdf\n",
    "def tl(thisdf,windown = 501,mad = 10,\n",
    "       psCol = 'diameterThisIter',tsCol = 'pupil_timestamp',psColNew = 'diameter_3dNew'):\n",
    "    #make trendline by interpolating and smoothing\n",
    "    cs = CubicSpline(thisdf[tsCol][~thisdf[psCol].isnull()].values,\n",
    "                   thisdf[psCol][~thisdf[psCol].isnull()].values)\n",
    "    interp = cs(thisdf[tsCol])\n",
    "    smoothed = smooth(interp,windown)\n",
    "    #tlmad = bool list\n",
    "    tlmad = mad_func(thisdf[psCol] -smoothed,mad)[0]\n",
    "    return tlmad\n",
    "def tlFinal(x,y,windown = 101,mad = 10):\n",
    "    #this is for doing trialwise tl rejection, not very necessay but since the input format\n",
    "    #is slightly different from the tl function, i'm including it as a sepearte one\n",
    "    cs = CubicSpline(x[~np.isnan(y)],\n",
    "                    y[~np.isnan(y)])\n",
    "    interp = cs(x)\n",
    "    smoothed = smooth(interp,windown)\n",
    "    #tlmad = bool list\n",
    "    tlmad = mad_func(y-smoothed,mad)[0]\n",
    "    return tlmad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#core functions\n",
    "def deBlink(rawPup,timestamp,\n",
    "            returnMask = False,\n",
    "            maxGapt = 2,\n",
    "            minGapt= 0.003,\n",
    "            freq = 1000,\n",
    "            borderlenBef=0.001,\n",
    "            borderlenAft=0.01,\n",
    "            clusterLapse = 0.15,\n",
    "            plotSteps = False,\n",
    "            madThreshold = 3.5,\n",
    "            minPupAllowed = 2000\n",
    "            ):\n",
    "    \"\"\"\n",
    "    #this function clean the pupil array, and generate a boolean blink mask array\n",
    "    @ rawPup = each row in the epoched pupil table\n",
    "    @ timestamp = \n",
    "    @ returnMask: if true, return the boolean mask only (this is necessday because df.apply can't return two dfs)\n",
    "    @ minGapt = \n",
    "    @ freq = \n",
    "    @ borderlenBef = paddings to add after each blink\n",
    "    @ borderlenAft = paddings to add before each blink\n",
    "    @ clusterLapse = larger value will count more distant blinks as one, resulting in more data rejected\n",
    "    @ plotSteps = if True, generate a plot for the cleaning step\n",
    "    @ madThreshold = speed threshold for rejection\n",
    "    @ minPupAllowed = the min pupil size allowed for it to not be classified as err\n",
    "    \"\"\"\n",
    "    #find the raw rej arr; This use a smoothed pupil change speed array to find the threshold\n",
    "    #crossings\n",
    "    rejarr = mad_func(find_abs_speed_pup(rawPup),madThreshold)[0] | (rawPup < minPupAllowed)\n",
    "\n",
    "    #initialize two mask array, these will store the processed blink mask\n",
    "    rejarr_padded = np.zeros(len(rejarr))\n",
    "    rejarr_blkMask = np.zeros(len(rejarr))\n",
    "\n",
    "    #pad the mask\n",
    "    try:\n",
    "        np.put(rejarr_padded, np.concatenate(reject_outlier(timestamp, rejarr, minGapLen= minGapt,freq = freq,\n",
    "                                                             borderlenBefore=borderlenBef,\n",
    "                                                    borderlen=borderlenAft)), 1, mode='clip')\n",
    "    except ValueError:\n",
    "        rejarr_padded = rejarr\n",
    "\n",
    "    #identify clusters in the mask (if two blink are too close, count them as one cluster and mask off the lapse between)\n",
    "    #put the resulting mask into rejarr_blkMask, this will be returned as output\n",
    "    try:\n",
    "        consecutiveBlkFilter = np.array([np.arange(i[0], i[-1]+1) for i in consecutive(np.where(rejarr_padded)[0], \n",
    "                                                                                       stepsize= clusterLapse*freq)])\n",
    "        np.put(rejarr_blkMask, np.concatenate(consecutiveBlkFilter), 1, mode='clip')\n",
    "    except ValueError:\n",
    "        rejarr_blkMask = rejarr_padded\n",
    "    except IndexError: \n",
    "        return rawPup\n",
    "        \n",
    "    blkMask = rejarr_blkMask.copy()\n",
    "\n",
    "    #just return the mask if necessary\n",
    "    if returnMask:\n",
    "        return blkMask\n",
    "\n",
    "    \"\"\"clean\"\"\"\n",
    "    #use this mask to clean the pupil, mask off the data in blink\n",
    "    #creating a copy for the plot because we will change the value of maskedPup \n",
    "    maskedPup_plt = np.where(rejarr_blkMask, np.nan, rawPup) \n",
    "    maskedPup = maskedPup_plt.copy() \n",
    "\n",
    "    \"\"\"interpolation\"\"\"\n",
    "    # reshape the clusted blink mask into arrays of clusted index, if its lenth if larger than the \n",
    "    # maximum gap lenth * frequency, then don't interpolate for this cluster, just leave it blank\n",
    "    rejCluster = consecutive(np.where(rejarr_blkMask)[0], 1)\n",
    "    rejCluster = np.array([k for k in rejCluster if len(k) < maxGapt*freq])\n",
    "\n",
    "    #find four index for interpolation\n",
    "    indexForInterp = np.array([np.array([max(0, i[0] - len(i)),\n",
    "                                        max(0, i[0]-1),\n",
    "                                        min(i[-1]+1, len(timestamp)-1),\n",
    "                                        min(len(timestamp)-1, i[-1] + len(i))]) for i in rejCluster])\n",
    "    \n",
    "    #when applying this function to a df, sometime it required the input to be in specific format\n",
    "    try:\n",
    "        timestamp = timestamp.values\n",
    "        rawPup = rawPup.values\n",
    "    except AttributeError:\n",
    "        timestamp = np.array(timestamp)\n",
    "        rawPup = np.array(rawPup)\n",
    "    \n",
    "    #run the following if there there are available blink clusters for interpolation, interpolate\n",
    "    try:\n",
    "        #get the timestamp and pupil size (masked) for creating the interpolation function\n",
    "        tsForInterp = timestamp[indexForInterp]\n",
    "        cleanPupForInterp = maskedPup[indexForInterp]\n",
    "        \n",
    "        #get the timestemp for exptrapolate the missing pupil size\n",
    "        tsForExtrap = [timestamp[i] for i in rejCluster]\n",
    "        \n",
    "        #for each available blink cluster,try use the cubic spline interpolation of 4 points\n",
    "        #but if any point if nan, then use linear; but if it still can't (e.g. only one data point availble)\n",
    "        #then don't interpolate, leave it as nan\n",
    "        for i in range(len(rejCluster)):\n",
    "            try:\n",
    "                pupExtrapolated = CubicSpline(tsForInterp[i], cleanPupForInterp[i])(tsForExtrap[i].astype(float))\n",
    "            except ValueError:\n",
    "                try:  # if cant find reference points, use linear interpolation\n",
    "                    pupExtrapolated = interp1d(tsForInterp[i][1:3], cleanPupForInterp[i][1:3])(tsForExtrap[i].astype(float))\n",
    "                # if still gives error, then dont interpolate (usually the case with trial start/end)\n",
    "                except ValueError:\n",
    "                    pupExtrapolated = np.nan\n",
    "            #fill each blink cluster with interpolated pupil value\n",
    "            maskedPup[rejCluster[i]] = pupExtrapolated\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "    if plotSteps:\n",
    "        fig, ax = plt.subplots(nrows=5)\n",
    "        #plot speed and threshold of rejection\n",
    "        sns.scatterplot(x= timestamp, y= find_abs_speed_pup(rawPup), ax=ax[0])\n",
    "        ax[0].hlines(mad_func(find_abs_speed_pup(rawPup), 3.5)[1],\n",
    "                    xmin=timestamp[0], xmax=timestamp[-100], colors='r')\n",
    "        #plot the unfilted mask\n",
    "        sns.scatterplot(x=timestamp, y=rawPup, hue=rejarr, ax=ax[1])\n",
    "        #plot the filtered mask\n",
    "        sns.scatterplot(x=timestamp, y=rawPup, hue=rejarr_blkMask, ax=ax[2])\n",
    "        #plot the pupil data after it's been masked\n",
    "        sns.scatterplot(x=timestamp, y=maskedPup_plt, ax=ax[3])\n",
    "        #plot the interpolated pupil size\n",
    "        sns.scatterplot(x=timestamp, y=maskedPup, ax=ax[4])\n",
    "        \n",
    "        ax[1].sharex(ax[0])\n",
    "        ax[2].sharex(ax[0])\n",
    "        ax[3].sharex(ax[0])\n",
    "        ax[4].sharex(ax[0])\n",
    "\n",
    "    \n",
    "    return maskedPup\n",
    "def findGazeShift(xvec,threshold = 6, countTogether = 50, \n",
    "                   baseline = None, winlen = 50,shiftmin = 2.14,\n",
    "                   baselinePadding = 0):\n",
    "    \"\"\"\n",
    "    @ xvec =  the gaze position (it should be each row in the saccRetro_X df)\n",
    "    @ threshold = the velocity filter threshold\n",
    "    @ countTogether = the maximum delay for two gaze shift cluster to be count as one\n",
    "    @ baseline = if none, calculate relative baseline  (i.e. in regard to position before and after shift)\n",
    "                else it should be a list of index to indicate baseline rage, e.g. [500,1000]\n",
    "    @ winlen = for the onset of each identified gazeshift cluster, get the [-winlen,0] and [winlen, 2+ winlen]\n",
    "            to calculate the mean gaze position before and after the shift occur\n",
    "    @ shiftmin = the minimum shift in gaze position for it to be considered as a real shift\n",
    "            default = 2.14 pixel = 0.057 visual degree\n",
    "    @ baselinePadding = the padding to add before the identified gaze shift, this is only meaningful if baseline\n",
    "                        is None (i.e. the shift regarding to gaze position before and after)\n",
    "    \n",
    "    \"\"\"\n",
    "    # get speed vector\n",
    "    # since we are using 4 time points to calculate the speed array (see find abs speed sacc function)\n",
    "    # the len of dx, dy is 4 points less than the length of xvec. \n",
    "    dx = find_abs_speed_sacc(xvec)\n",
    "    #get threshold, then get a boolean array, for finding threshold crossing clusters\n",
    "    thresholdx = [np.nanmedian(dx) + medianVariance(dx) *threshold,np.nanmedian(dx) - medianVariance(dx) *threshold]\n",
    "    cx = (dx>thresholdx[0]) | (dx < thresholdx[1])\n",
    "    cx = np.r_[False,False,cx,False,False] #correct for the four lost points\n",
    "\n",
    "\n",
    "    # Deselect period at the beginning and the end of the position array i.e.掐头去尾\n",
    "    # and re-identify the saccade cluster after applying both masks\n",
    "    # if two clusteres are close enough (50ms), then count them as one\n",
    "    onsetoffsetmask = np.r_[np.zeros(winlen + baselinePadding),np.ones(len(cx) - 3*winlen - 2* baselinePadding),np.zeros(2*winlen + baselinePadding)].astype(bool) #to deselect data in the first and last 50ms \n",
    "    maskedcx = np.where(cx & onsetoffsetmask)[0] \n",
    "    msCluster = consecutive(maskedcx,countTogether)\n",
    "\n",
    "    # initialize the output array with zeros, calculate the mean pos before and after the shift\n",
    "    gazeshift = np.zeros(len(xvec))    \n",
    "\n",
    "    try:\n",
    "        for i in msCluster:\n",
    "            #if user doesn't provide a baseline value, use relative baseline\n",
    "            if baseline == None:\n",
    "                befIndex = [i[0] - winlen - baselinePadding,i[0]- baselinePadding]\n",
    "            else:\n",
    "                befIndex = baseline\n",
    "            try:\n",
    "                posbef = np.nanmean(xvec.iloc[befIndex[0] : befIndex[1]])\n",
    "                posaft = np.nanmean(xvec.iloc[i[0] + winlen + baselinePadding : i[0] + 2*winlen + baselinePadding]) \n",
    "\n",
    "            except AttributeError: # this is to account for when using df.apply(raw = True),xvec is an array, not series\n",
    "                posbef = np.nanmean(xvec[befIndex[0] : befIndex[1]])\n",
    "                posaft = np.nanmean(xvec[i[0] + winlen + baselinePadding: i[0] + 2*winlen + baselinePadding]) \n",
    "\n",
    "            if abs(posaft - posbef) > shiftmin: \n",
    "                #toss away a ms if its less than a minimum visual degree \n",
    "                gazeshift[i[0]] = posaft - posbef \n",
    "            else:\n",
    "                pass\n",
    "    except IndexError: #i.e. if msCluster is empty, it gives error when trying to index i[0], in this case just return the 0 array\n",
    "        return gazeshift\n",
    "    return gazeshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#this is originally used to linearly interpolate \n",
    "#missing saccade data\n",
    "def linearInterpolate(x,y,method = 'linear'):\n",
    "    \"\"\"\n",
    "    methods needs to be one of the scipy.interpolate.interp1d methods\n",
    "    https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html\n",
    "    \"\"\"\n",
    "    #if the entire trial is nan, return nan\n",
    "    if np.isnan(np.array(y)).sum() == len(y):\n",
    "        interp = np.empty(len(y))\n",
    "        interp[:] = np.nan\n",
    "        return interp\n",
    "    #else,interpolate\n",
    "    try:\n",
    "        li = interp1d(x[~y.isnull()],\n",
    "                        y[~y.isnull()],\n",
    "                        fill_value=\"extrapolate\",kind = method)\n",
    "        interp = li(x)\n",
    "    except AttributeError: #handling np.array and pandas df conversion\n",
    "        li = interp1d(x[~np.isnan(y)],\n",
    "                        y[~np.isnan(y)],fill_value=\"extrapolate\",kind = method)\n",
    "        \n",
    "    interp = li(x)\n",
    "    return interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting function\n",
    "def legend_without_duplicate_labels(ax,locVal = 'best'):\n",
    "    #https://stackoverflow.com/questions/19385639/duplicate-items-in-legend-in-matplotlib\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    unique = [(h, l) for i, (h, l) in enumerate(zip(handles, labels)) if l not in labels[:i]]\n",
    "    ax.legend(*zip(*unique),loc = locVal)\n",
    "def deBlinkSample(rawPup, samplesize = 10): #plot n trial samples with the deblink process\n",
    "    fs(12,12)\n",
    "    select = np.random.randint(0,len(rawPup),size = samplesize)\n",
    "\n",
    "    #select some smaple \n",
    "    sample_raw = rawPup.iloc[select,1:]\n",
    "\n",
    "    #Clean once\n",
    "    cleanOnceSample = sample_raw.apply(lambda x: deBlink(rawPup = x.astype(float),timestamp = np.linspace(0,5,len(x)),\n",
    "                                                clusterLapse = 0.05, borderlenAft = 0.2,\n",
    "                                                borderlenBef= 0.05,madThreshold = 4),axis = 1,raw = True)\n",
    "\n",
    "    #second clean\n",
    "    onceSmoothSample = cleanOnceSample.rolling(11,min_periods=1,center  = True,axis =1).mean()\n",
    "    cleanTwiceSample = onceSmoothSample.apply(lambda x: deBlink(rawPup = x,timestamp = np.linspace(0,5,len(x)),\n",
    "                                            borderlenBef = 0.05,borderlenAft = 0.05,minGapt = 0.002,\n",
    "                                            clusterLapse = 0.05,madThreshold = 6),axis =1,raw = True)\n",
    "    fig,ax = plt.subplots(nrows = 3)\n",
    "\n",
    "    x = np.linspace(0,5,5000)\n",
    "    [ax[0].plot(x,sample_raw.iloc[i,:],alpha = 0.8) for i in range(len(select))]\n",
    "    [ax[1].plot(x,cleanOnceSample.iloc[i,:],alpha = 0.84,linewidth = 2) for i in range(len(select))]\n",
    "    [ax[2].plot(x,cleanTwiceSample.iloc[i,:],alpha = 0.8,linewidth = 2) for i in range(len(select))]\n",
    "\n",
    "    ax[1].set_ylim(ax[2].get_ylim())\n",
    "    ax[0].set_ylim(ax[2].get_ylim())\n",
    "\n",
    "    plt.show()\n",
    "def pupilPlot(normPup,blPup, \n",
    "              eventt = 3,\n",
    "              blt  = 0.1,\n",
    "              freq = 1000,\n",
    "              type = 'grand',\n",
    "              levels = ['cue_blk_validity','cued_item'],\n",
    "              pltRatio = [0.2,3], pltN = 3,\n",
    "              eventBoundary = [0,0.5,2.5],\n",
    "              eventName = ['retrocue','delay','probe'],\n",
    "              blHeight = None,\n",
    "              mainFigHeight = None,\n",
    "              cList = [],\n",
    "              figH = 5.5, figW = 16,\n",
    "              tightPlot = True,\n",
    "              textSize = 12,\n",
    "              sigPeriod = None,\n",
    "              sigAx = 0,\n",
    "              sigBarHeight = 30,\n",
    "              textOffset = 10,\n",
    "              tickSize = 10\n",
    "              ):\n",
    "    \"\"\"\n",
    "    @ normPup = normalized pupil df with trial info columns in the end\n",
    "    @ blPup = the pupil df with the baseline\n",
    "    @ eventt = the time for the event of interest\n",
    "    @ blLen = the time for the baseline\n",
    "    @ freq = sampling frequency, defalut 1000\n",
    "    @ type = the type of plot, \n",
    "           - grand avg: ;\n",
    "           - subj avg: ;\n",
    "    @ levels = the group by levels, e.g. block validity and cued item\n",
    "    @ pltRatio = ratio for the plot for baseline and event\n",
    "    @ pltN = how many plots, default 3\n",
    "    @ eventBoundary = use to plot vertical lines on the plot to represent certain events\n",
    "    @ eventName = the text label for these events\n",
    "    @ blHeight = ylim for the baseline plots, if None, set ylim using max and min\n",
    "    @ mainFigHeight = ylim for the main plots, if None, set ylim using max and min\n",
    "    @ cList = the color list used for plots; default color- the ones i carefully preseletced, very pretty\n",
    "    @ figH = figure height for each plot, default 5.5\n",
    "    @ figW = figure width, default 16\n",
    "    @ tightPlot = if true, set the plot layout to be tighter\n",
    "    @ textSize = the text label on the plot size\n",
    "    @ sigPeriod = 2 item list [start, end]\n",
    "    @ sigAx = axes to draw the sigbar\n",
    "    @ sigBarHeight = height of the significant bar, the larger the thicker\n",
    "    @ textOffset = the height of text offset, the larger , the lower the text\n",
    "    @ tickSize = size for ticks\n",
    "    --------------\n",
    "    return NormMean: the groupby df\n",
    "    \"\"\"\n",
    "    eventLen = int(eventt*freq)\n",
    "    blLen = int(blt*freq)\n",
    "\n",
    "    if not type in ['grand','subj']:\n",
    "        raise 'Your plot should either be grand avg or subj avg'\n",
    "    if len(eventBoundary) != len(eventName):\n",
    "        raise 'your event name array should be the same size as your event boundary(time) array'\n",
    "    if type == 'grand':\n",
    "        normmean = normPup.groupby(levels).mean().iloc[:,:eventLen]\n",
    "        norm_sterr = normPup.groupby(levels).sem().iloc[:,:eventLen]\n",
    "        norm_upper = normmean.iloc[:,:] +norm_sterr.iloc[:,:] \n",
    "        norm_lower = normmean.iloc[:,:] -norm_sterr.iloc[:,:] \n",
    "\n",
    "        normmeanBL = blPup.groupby(levels).mean().iloc[:,:blLen]\n",
    "        norm_sterrBL = blPup.groupby(levels).sem().iloc[:,:blLen]\n",
    "        norm_upperBL = normmeanBL.iloc[:,:] +norm_sterrBL.iloc[:,:] \n",
    "        norm_lowerBL = normmeanBL.iloc[:,:] -norm_sterrBL.iloc[:,:]\n",
    "\n",
    "        #the mean df will be returned\n",
    "        returnMean = normmean    \n",
    "        returnSd = norm_sterr\n",
    "    elif type == 'subj':\n",
    "        subj_levels = levels + ['subj']\n",
    "        normmean = normPup.groupby(subj_levels).mean().iloc[:,:eventLen].groupby(levels).mean()\n",
    "        norm_sterr = normPup.groupby(subj_levels).mean().iloc[:,:eventLen].groupby(levels).sem()\n",
    "        norm_upper = normmean.iloc[:,:] +norm_sterr.iloc[:,:] \n",
    "        norm_lower = normmean.iloc[:,:] -norm_sterr.iloc[:,:] \n",
    "        \n",
    "        normmeanBL = blPup.groupby(subj_levels).mean().iloc[:,:blLen].groupby(levels).mean()\n",
    "        norm_sterrBL = blPup.groupby(subj_levels).mean().iloc[:,:blLen].groupby(levels).sem()\n",
    "        norm_upperBL = normmeanBL.iloc[:,:] +norm_sterrBL.iloc[:,:] \n",
    "        norm_lowerBL = normmeanBL.iloc[:,:] -norm_sterrBL.iloc[:,:]\n",
    "\n",
    "        #the subj df will be returned\n",
    "        returnMean = normPup.groupby(subj_levels).mean().iloc[:,:eventLen]\n",
    "        returnSd =  normPup.groupby(subj_levels).sem().iloc[:,:eventLen]\n",
    "\n",
    "    condN = normmean.shape[0]\n",
    "    #if color list is not right length, use sns generated color\n",
    "    if len(cList) < condN:\n",
    "        print('input color list is too short, plot default color ')\n",
    "        cList = sns.color_palette(\"flare\",n_colors = condN)\n",
    "     \n",
    "    #setting up plotting params\n",
    "    fs(figW,pltN*figH) #set figure size\n",
    "    fig,ax = plt.subplots(nrows= pltN, ncols=2, sharex=False, sharey = False,\n",
    "                          gridspec_kw={'width_ratios': pltRatio,})\n",
    "    x = np.linspace(0,eventt,norm_sterr.shape[1])\n",
    "    xBL = np.linspace(0,blt,norm_sterrBL.shape[1])\n",
    "\n",
    "    #height parameters\n",
    "    if blHeight == None:\n",
    "        blHeight = [norm_lowerBL.min().min(), norm_upperBL.max().max()]\n",
    "    if mainFigHeight == None:\n",
    "        minH = norm_lower.min().min()\n",
    "        maxH = norm_upper.max().max()\n",
    "    \n",
    "    #set height\n",
    "    [ax[i][0].set_ylim(blHeight) for i in range(pltN)]\n",
    "    if mainFigHeight != None:\n",
    "\n",
    "        [ax[i][1].set_ylim(mainFigHeight) for i in range(pltN)]\n",
    "        minH = mainFigHeight[0]\n",
    "        maxH = mainFigHeight[1]\n",
    "\n",
    "    #plot mean plot for the baseline and for the trial\n",
    "    [ax[i//2][0].fill_between(xBL,norm_upperBL.iloc[i,0:norm_sterrBL.shape[1]],norm_lowerBL.iloc[i,0:norm_sterrBL.shape[1]],\n",
    "                        color = cList[i],alpha = 0.2,)  for i in range(condN)]\n",
    "    [ax[i//2][0].plot(xBL,normmeanBL.iloc[i,0:norm_sterrBL.shape[1]],\n",
    "                      linewidth = 2.5,color = cList[i],) for i in range(condN) ]\n",
    "\n",
    "    [ax[i//2][1].fill_between(x,norm_upper.iloc[i,0:norm_sterr.shape[1]],norm_lower.iloc[i,0:norm_sterr.shape[1]],\n",
    "                        color = cList[i],alpha = 0.1, )  for i in range(condN)]\n",
    "    [ax[i//2][1].plot(x,normmean.iloc[i,0:norm_sterr.shape[1]],\n",
    "                      color = cList[i],linewidth = 3,) for i in range(condN) ]\n",
    "    \n",
    "    if sigPeriod != None:\n",
    "        ax[sigAx][1].hlines(0,x[sigPeriod[0]],x[sigPeriod[1]],colors = cList[0],\n",
    "           linewidth = sigBarHeight)\n",
    "    \n",
    "    #add vertical line and text\n",
    "    for ff in range(pltN):\n",
    "        ax[ff][1].vlines(eventBoundary,minH, maxH, linestyles = 'dashed',colors= 'black')\n",
    "    for i,m in zip(eventBoundary,eventName):\n",
    "        ax[0][1].text(i,maxH - textOffset,m,c = 'black',size = textSize)\n",
    "    [ax[i][j].tick_params( labelsize=tickSize) for i in range(pltN) for j in range(2)]   \n",
    "\n",
    "    [ax[i][1].yaxis.set_major_locator(ticker.MultipleLocator(50)) for i in range(3)]\n",
    "\n",
    "    #make the plots tighter\n",
    "    if tightPlot:\n",
    "        plt.subplots_adjust(hspace = 0.1)\n",
    "        plt.subplots_adjust(wspace = 0.06)\n",
    "    \n",
    "    #plt.xticks(fontsize= figW, )\n",
    "    plt.show()\n",
    "\n",
    "    return returnMean,returnSd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for behavioral response from psychopy\n",
    "def find_click_time(click,time):\n",
    "    try:\n",
    "        rt = time[np.nonzero(click)[0][0]]\n",
    "    except IndexError:\n",
    "        rt = np.nan\n",
    "    return rt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the mega df\n",
    "local_username = 'yud070'\n",
    "pathEL = 'Z:/LiteBrite_YueyingDong/22AK01ELRaw_tracker/'\n",
    "pathpy = 'Z:/LiteBrite_YueyingDong/22AK01ELRaw_psychopy/'\n",
    "\n",
    "readIn = [f for f in os.listdir(pathEL)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevantCols = ['RECORDING_SESSION_LABEL','AVERAGE_GAZE_X',\n",
    "       'AVERAGE_GAZE_Y','AVERAGE_PUPIL_SIZE','EYE_TRACKED','IP_LABEL','IP_START_TIME',\n",
    "      'LEFT_PUPIL_SIZE','RIGHT_PUPIL_SIZE','SAMPLE_MESSAGE','TIMESTAMP','TRIALID']\n",
    "pupCols = ['EYE_TRACKED','LEFT_PUPIL_SIZE','RIGHT_PUPIL_SIZE','AVERAGE_PUPIL_SIZE',\n",
    "           'IP_LABEL','TIMESTAMP','TRIALID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting params // pending refractor\n",
    "ts = [1000,1250,1750,2250,4250,4750] #stim onset, rest, retrocue, delay, probecue\n",
    "\n",
    "stim_ts = 0.25 #+ fixation_ts\n",
    "precue_ts = 0.5 + stim_ts\n",
    "retrocue_ts = 0.5 + precue_ts\n",
    "post_cue_ts = 2 + retrocue_ts\n",
    "probe_cue = 0.5 + post_cue_ts\n",
    "probe_ts = 3 + post_cue_ts\n",
    "\n",
    "epoch_array = np.array([0,stim_ts,precue_ts,retrocue_ts,post_cue_ts,probe_cue])\n",
    "epoch_name = ['stim','rest','retrocue','rest','probe_cue','probe']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#events\n",
    "events = ['fixateRest','stim','precueRest', 'retrocue',\n",
    "                            'postcueRest','probecue', 'probe', 'ITI','trialEnd']\n",
    "eventIndex = [0,1000,1250, 1750,2250,4250,4750,7750,8250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take the first 5s in each recording, though the interest period is [0.5,4.75], but the padding\n",
    "#is included for better interpolating\n",
    "\n",
    "#params for reading in\n",
    "included = 5000\n",
    "\n",
    "#params for cleaning\n",
    "sdThreshold = 6\n",
    "pltAll = True\n",
    "interpCorrection = np.zeros(len(readIn))\n",
    "maxGapLen = 0.5\n",
    "minGapLen = 0.0\n",
    "freq = 1000\n",
    "gapMinN = minGapLen*freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for plotting\n",
    "cList = ['#fda48c','#f23908','#93648f','#33202a','#66b3ba','#03838f',]\n",
    "cDict = {\"['high']\":'#f23908',\"['medium']\":'#03838f',\"['low']\":'#593247'} "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# readIn "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### readin Psychopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the psychopy file that contains all subjects record\n",
    "psyFull = pd.read_csv('Z:/LiteBrite_YueyingDong/22AK01ELRaw_psychopy/allsubj.csv').iloc[:,1:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### readin Pupil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathUnparsed = 'Z:/LiteBrite_YueyingDong/temp/unparsed/'\n",
    "\n",
    "#readin the eyelink pupil file\n",
    "pupFullTrial_raw = pd.concat((pd.read_csv(f).iloc[:,1:] for f in glob.glob(pathUnparsed + 'pupRaw'+'/*.csv')), ignore_index=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### readin events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathUnparsed = 'Z:/LiteBrite_YueyingDong/temp/unparsed/'\n",
    "\n",
    "#readin the events file\n",
    "eventDf = pd.concat((pd.read_csv(f).iloc[:,1:] for f in glob.glob(pathUnparsed + 'event'+'/*.csv')), ignore_index=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find no response trials"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get rt from psychopy file, mouse click array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the left click arrary and the time array to identify the time of mouse first click\n",
    "psyFull['mouse_click'] = [ast.literal_eval(i) for i in psyFull['mouse.leftButton']]\n",
    "psyFull['mouse_time'] = [ast.literal_eval(i) for i in psyFull['mouse.time']]\n",
    "psyFull['rt'] = [find_click_time(i,n) for i,n in zip(psyFull['mouse_click'],psyFull['mouse_time'])]\n",
    "\n",
    "#add a column of block validity condition\n",
    "psyFull['blk_validity'] = [ast.literal_eval(i)[0] for i in psyFull.cue_blk_validity]\n",
    "#add a column of blk+trial condition\n",
    "psyFull['blk_trl_cond'] = psyFull.blk_validity+psyFull.cue_condition\n",
    "\n",
    "#if no click is made, the trial is considered to be \"missing response\"\n",
    "missing_ts = psyFull[(psyFull['mouse_click'].apply(sum) ==0) ].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count how many trials are missing for each subj\n",
    "missingRsp = missing_ts.participant.value_counts().reset_index(drop = False,name = 'missCnt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of the trials without response\n",
    "psyFull = psyFull[~psyFull.identifier.isin(missing_ts.identifier)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pupil"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean + smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean once\n",
    "cleanOnce = pupFullTrial_raw.iloc[:,1:].apply(lambda x: deBlink(rawPup = x.astype(float),timestamp = np.linspace(0,5,len(x)),\n",
    "                                              borderlenBef= 0.05, borderlenAft = 0.2,\n",
    "                                              clusterLapse = 0.05, madThreshold = 4),axis = 1,raw = True)\n",
    "\n",
    "#smooth the resulting array using rolling window of size 11\n",
    "onceSmooth = cleanOnce.rolling(11,min_periods=1,center  = True,axis =1).mean()\n",
    "\n",
    "#second clean, this is to get rid of any residual outlier\n",
    "cleanTwice = onceSmooth.apply(lambda x: deBlink(rawPup = x,timestamp = np.linspace(0,5,len(x)),\n",
    "                                        borderlenBef = 0.05,borderlenAft = 0.05,minGapt = 0.002,\n",
    "                                        clusterLapse = 0.05,madThreshold = 6),axis =1,raw = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset Index, add back the trial id\n",
    "pupFullTrial = cleanTwice.reset_index(drop = True)\n",
    "pupFullTrial.insert(0,'TRIALID',pupFullTrial_raw.TRIALID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also return the blink mask; the mask have a high positive rate, i set the mad threshold to be higher\n",
    "#to be more certain for blinks\n",
    "maskClean = pupFullTrial_raw.iloc[:,1:].apply(lambda x: deBlink(rawPup = x.astype(float),timestamp = np.linspace(0,5,len(x)),\n",
    "                                              borderlenBef= 0.05, borderlenAft = 0.2,\n",
    "                                              clusterLapse = 0.05, madThreshold = 6,returnMask = True),axis = 1,raw = True)\n",
    "maskClean.insert(0,'TRIALID',pupFullTrial_raw.TRIALID)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### correction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask off any pupil data that exceed a median +- n*medianSD threshold for that subject\n",
    "stdmask = pd.DataFrame()\n",
    "temp = pupFullTrial.merge(psyFull[['participant','identifier']],how = 'left',\n",
    "                   left_on='TRIALID', right_on='identifier')\n",
    "\n",
    "for i in temp.participant.unique():\n",
    "    tempdf = temp[temp.participant == i]\n",
    "\n",
    "    #calculate upper/lower bounds\n",
    "    stdupper = np.nanmedian(tempdf.iloc[:,1:included+1])  + sdThreshold* medianVariance(tempdf.iloc[:,1:included+1])\n",
    "    stdlower = np.nanmedian(tempdf.iloc[:,1:included+1])  - sdThreshold* medianVariance(tempdf.iloc[:,1:included+1])\n",
    "    #create std mask for this subj\n",
    "    stdmask_temp = (tempdf.iloc[:,1:included+1] > stdupper) | (tempdf.iloc[:,1:included+1] < stdlower)\n",
    "    stdmask_temp.insert(0,'TRIALID',tempdf.TRIALID)\n",
    "    \n",
    "    stdmask = pd.concat([stdmask,stdmask_temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no one cared who i was until I put on the df.mask\n",
    "# reject the invalid data that's outside of the threshold\n",
    "pupFullTrial.iloc[:,1:] = pupFullTrial.iloc[:,1:].mask(stdmask.iloc[:,1:],np.nan)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rejection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep the trial if it has more than 90% data retained in the period of interest\n",
    "#trialwise\n",
    "pupClean = pupFullTrial[(pupFullTrial.iloc[:,ts[1]:ts[-2]+1].isnull().sum(axis = 1)) < (0.125* 3000)] .reset_index(drop = True)\n",
    "pupClean = pupClean[~pupClean.TRIALID.isin(missing_ts.identifier)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subjwise rej, if more than 30% missing (90 trials), rej subj\n",
    "pupClean = pupClean.merge(psyFull[['participant','identifier',\n",
    "                                      'cue_blk_validity','cued_item',]],how = 'left',\n",
    "                   left_on='TRIALID', right_on='identifier')\n",
    "#count how many trials each subj still have \n",
    "trlRemain = pupClean.participant.value_counts().reset_index().rename(columns={'index':'participant',\n",
    "                                                                              'participant':'cnt'})\n",
    "\n",
    "#also consider the rejection base on behavioral response\n",
    "trlRemain['pupRejCnt'] = 300 - trlRemain.cnt\n",
    "trlRemain = trlRemain.merge(right = missingRsp,how = 'outer',right_on = 'index',left_on = 'participant').replace(np.nan,0)\n",
    "trlRemain['rejCnt'] = trlRemain.pupRejCnt + trlRemain.missCnt\n",
    "\n",
    "#reject subject\n",
    "rejsubj = trlRemain[trlRemain.rejCnt > 0.3*300].participant\n",
    "pupClean = pupClean[~pupClean.participant.isin(rejsubj)].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also reject the subject for psychopy file\n",
    "readIn = pupClean.participant.unique()\n",
    "psyFull = psyFull[psyFull.participant.isin(readIn)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### see sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "deBlinkSample(rawPup =pupFullTrial_raw, samplesize = 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validity check, this will plot out the cleaned pupil for every subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load params\n",
    "subjtest = pupClean.participant.unique()\n",
    "indPlotSize = (12,6) #change this to adjust individual plot size\n",
    "\n",
    "#plot size params\n",
    "ny = 5\n",
    "nx = (len(subjtest)//ny) +1\n",
    "fs(indPlotSize[0] *ny,indPlotSize[1] *nx)\n",
    "x = np.linspace(0,5,5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(nrows= nx, ncols = ny)\n",
    "counter_x = 0\n",
    "counter_y = 0\n",
    "\n",
    "for f in readIn:\n",
    "    f = str(f)\n",
    "    subjdf = pupClean[(pupClean['participant'] == f)|\n",
    "                      (pupClean['participant'] == float(f))].reset_index(drop = True)\n",
    "    \n",
    "    maxpoint = subjdf.iloc[:,1:included+1].max().max()\n",
    "    minpoint = subjdf.iloc[:,1:included+1].min().min()\n",
    "    #plot\n",
    "    [ax[counter_x][counter_y].plot(x,subjdf.iloc[i,1:included+1],alpha = 0.4,\n",
    "                                   ) for i in range(len(subjdf))]\n",
    "    ax[counter_x][counter_y].set_title(f)\n",
    "\n",
    "    counter_x+=1\n",
    "    if counter_x == nx:\n",
    "        counter_y+=1\n",
    "        counter_x = 0\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# epoching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"parse using event\"\"\"\n",
    "\"\"\"\"\"\"\n",
    "# reset index for the files\n",
    "eventDfPup = eventDf[eventDf.TRIALID.isin(pupClean.TRIALID)].reset_index(drop = True)\n",
    "\n",
    "# sort to ensure the trial id match\n",
    "pupClean = pupClean.sort_values('TRIALID').reset_index(drop = True)\n",
    "eventDfPup = eventDfPup.sort_values('TRIALID').reset_index(drop = True)\n",
    "\n",
    "# sanity check, if this is not 0, stop running the following\n",
    "sum(pupClean.TRIALID != eventDfPup.TRIALID )\n",
    "\n",
    "# find the retrocue onset time\n",
    "# for the baseline period, select [-500, 100ms] regard to event onset\n",
    "# but for the subsequent normalization, we only use [-100,0ms]\n",
    "tsRetroBL = [np.arange(i- 500,i+ 100) for i in eventDfPup.retrocue]\n",
    "# find the onset time, then add 3000ms\n",
    "tsRetro = [np.arange(i,i + 3000) for i in eventDfPup.retrocue]\n",
    "\n",
    "#parse for retrocue/delay/probecue period 3000ms\n",
    "pupRetroBL = pd.DataFrame(data=[pupClean.iloc[x:x+1,y].values[0] for x,y in enumerate(tsRetroBL)])\n",
    "pupRetro = pd.DataFrame(data=[pupClean.iloc[x:x+1,y].values[0] for x,y in enumerate(tsRetro)])\n",
    "\n",
    "#put back the trial id\n",
    "pupRetroBL.insert(0,'TRIALID',pupClean.TRIALID)\n",
    "pupRetro.insert(0,'TRIALID',pupClean.TRIALID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some params\n",
    "baselineLen = 100\n",
    "baselineSdThreshold = 2.5\n",
    "x = np.linspace(0,5000,5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if any nan occured during the baseline interval, reject the trial, 2 trials are rejected\n",
    "retroBLrej = pupRetroBL[pupRetroBL.iloc[:,401:401+baselineLen].isnull().sum(axis = 1)>1].TRIALID\n",
    "retroEpo = pupRetro[~pupRetro.TRIALID.isin(retroBLrej)].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the periof of -100-0 ms prior to cue onset \n",
    "#(in this dataframe of [-500,100ms] it should correspond to [400,500] column, plus the 1 identifier column)\n",
    "pupRetroBL['u'] = pupRetroBL.iloc[:,401:401+baselineLen].mean(axis = 1) #take mean\n",
    "retroEpoClean = retroEpo.merge(pupRetroBL[['TRIALID','u']],how = 'inner',on = 'TRIALID')\n",
    "\n",
    "#set the identifier as dataframe index\n",
    "pupRetroBL.set_index('TRIALID',inplace=True)\n",
    "retroEpoClean.set_index('TRIALID',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "retroEpoClean.iloc[:,:3000] = retroEpoClean.iloc[:,:3000].sub(retroEpoClean.iloc[:,-1],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add condition info (i.e. the block reliability, cued item brightness, and subjectID)\n",
    "retroEpoClean = retroEpoClean.iloc[:,:].merge(psyFull[['identifier','cue_blk_validity','cued_item','participant']],\n",
    "                        right_on = 'identifier' ,left_index= True,how = 'inner').rename(columns = {'participant':'subj'})\n",
    "retroEpoBL = pupRetroBL.iloc[:,400:500].merge(psyFull[['identifier','cue_blk_validity','cued_item','participant']],\n",
    "                        right_on = 'identifier' ,left_index= True,how = 'inner').rename(columns = {'participant':'subj'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the psychopy with missing response trials rejected\n",
    "psyFull.to_csv('Z:/LiteBrite_YueyingDong/temp/cleanBehav.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the blink mask\n",
    "maskClean.to_csv('Z:/LiteBrite_YueyingDong/data4Paper/mask/cleanMask.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean, epoched pupil data\n",
    "retroEpoClean.to_csv('Z:/LiteBrite_YueyingDong/data4Paper/cleanPup_parsedByEventMarker_normalized/cleanPupRetroNorm_byEvents.csv')\n",
    "retroEpoBL.to_csv('Z:/LiteBrite_YueyingDong/data4Paper/cleanPup_parsedByEventMarker_normalized/cleanPupRetroBL_byEvents.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
